Metatron Engine (Druid) 소개
------------------------------------------------

Overview
===================================
근래에 들어 정보통신 기술의 발전과 함께 데이터 발생량이 비약적으로 증가하였고, 이를 효율적으로 수집·관리·활용하는 것의 중요성이 대두되고 있다. 하지만 RDBMS 위주의 레거시 시스템들로는 대용량의 다차원 데이터를 온전히 처리할 수가 없기 때문에 차세대 빅데이터 수요를 충족시키기 위한 새로운 방법론과 솔루션들이 대거 등장하기 시작하였다.
미국 실리콘 밸리에 소재한 스타트업 Metamarkets사에서는 2011년 Druid라는 컬럼 기반 분산형 데이터 스토어를 출시하였고 2012년 10월 이를 오픈소스로 전환하였는데, 빠르고 효율적인 데이터 처리를 비롯한 여러 장점 때문에 많은 기업에서 Druid를 backend 기술로 활용하고 있다.
이러한 흐름에서 SK텔레콤도 B2C 이동통신 서비스 제공업체로서 매순간 이용자들로부터 발생하는 엄청난 양의 네트워크 데이터를 효과적으로 관리·분석할 필요가 있었으며, 2016년 Druid를 기본 엔진으로 활용한 end-to-end 비즈니스 인텔리전스 솔루션 metatron을 개발·출시하였다.
본서에서는 시계열 데이터 처리에 적합한 Druid의 특징에 대해 알아본 후 SK텔레콤의 metatron에서 이를 어떤 식으로 적용하고 추가 기능들을 개발하였는지 소개한다. Druid에 대한 소개에 해당하는 2~3장은 Druid 웹사이트1와 기타 공식 Druid 자료들2-6을 토대로 작성하였다.


Druid 개발 배경
===================================
Druid는 대량의 트랜잭션 이벤트(로그 데이터)를 ingestion하고 탐색할 수 있도록 지원하는 엔진으로서 다음과 같은 니즈를 충족하기 위해 개발되었다.
첫째, 개발자들은 어떠한 차원들의 조합에 대한 쿼리라도 즉각적으로 결과가 반환되며, 데이터를 신속히 임의로 slice 및 dice하고 이를 아무 제약 없이 효과적으로 drill down하는 기능을 구현하고자 했다. 이러한 기능들은 사용자들이 데이터 대시보드에서 이벤트 스트림들을 interactive한 방식으로 자유롭게 탐색하고 시각화하는 데 필요하였다.
둘째, 개발자들은 이벤트들이 발생하는 즉시 해당 데이터를 ingestion하여 쿼리 가능하도록 인덱싱하는 기능을 원했다. 이러한 기능은 사용자들이 데이터를 실시간으로 수집·분석함으로써 시의적절하게 상황을 판단·예측하고 비즈니스 의사 결정을 내릴 수 있도록 하는 데 반드시 필요했다. 당시에 Hadoop 등의 유명한 오픈소스 데이터 웨어하우스 시스템들은 sub-second 데이터 ingestion이 불가능했다.
마지막으로, 개발자들은 multitenancy와 high availability를 보장하고자 했다. 이들이 제공하는 서비스의 기반 시스템은 많은 사용자들의 동시 접속을 보장하고, 항상 가동 상태를 유지하며 downtime 없이 모든 고장에 견딜 수 있어야 했다. downtime의 존재는 비용을 발생시키며, 소프트웨어 업그레이드나 네트워크 장애 발생 시 사용할 수 없는 시스템으로는 많은 기업의 비즈니스가 불가능하다.



오픈소스 Druid 특징
===================================

데이터 테이블 형태


Druid의 데이터 테이블(Druid에서는 ‘데이터 소스’라고 함)은 OLAP 쿼리용으로 설계된 시계열 이벤트들로 구성된다. 데이터 소스는 세 종류의 컬럼으로 구성된다(여기서는 온라인 광고 데이터를 예시로 사용).

* 타임스탬프 컬럼: Druid는 데이터 소스에서 타임스탬프 컬럼을 별도로 구성함으로써 모든 쿼리가 시간 축을 중심으로 이루어지게 한다. (시계열 속성이 없는 데이터를 일괄적으로 ingestion할 경우에는 현재 시간을 기준으로 타임스탬프가 부여되어 Druid에서 활용할 수 있는 형태가 된다.)
* 차원 컬럼: 차원 컬럼은 각 이벤트의 문자열 속성들을 담고 있으며, 데이터 필터링 시 가장 흔히 사용된다. 위 데이터셋 예시에서는 publisher, advertiser, gender, country가 차원 컬럼이다. 데이터 탐색 시에는 이러한 차원 컬럼들을 축으로 하여 데이터를 slice하게 된다.
* 측정값 컬럼: 측정값 컬럼들은 집계 및 연산에 사용된다. 위 예에서는 clicks 및 price가 측정값 컬럼이다. 측정값 컬럼의 자료형은 대체로 수치 값이며, 이들은 계수, 합산, 평균 등의 방식으로 집계할 수 있다(metatron에서는 지원되는 자료형을 증대하였다).


데이터 ingestion

Druid는 실시간 및 일괄(batch) ingestion을 지원한다.
이 중에서 실시간 ingestion은 Druid의 주요 특징 중 하나인데, 이를 전담하는 real-time 노드군이 있기 때문에 가능한 것이다(자세한 설명은 3.7.1절 ‘Real-time 노드’ 참조). 실시간으로 ingestion되는 데이터 스트림 내 이벤트들은 발생 후 수 초 이내에 Druid 클러스터에서 쿼리가 가능한 포맷으로 인덱싱된다.


데이터 roll-up

무수히 많은 개별 이벤트를 단순히 열거하기만 해서는 중요한 의미를 찾을 수 없다. 하지만 이러한 데이터를 적절한 시간대를 기준으로 취합하면 유용한 인사이트를 얻을 수 있다. Druid는 roll-up이라는 옵션을 통해 ingestion되는 원천 데이터를 취합할 수 있다. 아래는 roll-up의 예시를 나타낸 것이다.

왼쪽의 원본 이벤트 목록은 2011년 1월 1일 00:00:00~01:00:00 사이에 발생한 도메인 클릭 이벤트를 열거한 것이다. 하지만 분석가 입장에서는 분 이하 단위의 개별 이벤트가 별다른 의미를 갖지 못하기 때문에 1시간의 granularity를 기준으로 데이터를 취합했다. 그 결과 오른쪽 테이블에 나타난 것처럼 2011년 1월 1일 00~01시 시간대에 각 도메인을 남성과 여성이 각각 클릭한 횟수를 보여주는 보다 의미 있는 결과물이 도출되었다.
또한 데이터 roll-up은 원천 데이터의 저장 용량을 최소함으로써(많게는 100배까지도 축소 가능), 스토리지 리소스를 절약하고 쿼리 속도를 빠르게 한다.
그러나 데이터를 roll-up하면 개별 이벤트들에 대해 쿼리할 수 없게 된다. roll-up granularity는 데이터를 탐색할 수 있는 최소 단위가 되며 이벤트들은 이러한 granularity 단위로 배열된다. granularity 단위는 사용자가 원하는 대로 설정할 수 있으며, 원치 않으면 roll-up을 비활성화하고 모든 개별 이벤트를 전부 ingestion할 수도 있다.


데이터 sharding

데이터 소스는 시계열 이벤트들의 집합으로서 여러 shard로 분할 저장되는데, Druid에서는 이를 ‘세그먼트’라고 부르며 각 세그먼트는 대개 500~1,000만 행으로 이루어진다. Druid는 데이터 소스들을 정의된 시간 간격(통상적으로 1시간이나 하루)을 기준으로 분할하며, 그 밖의 컬럼에 있는 값들을 기준으로 추가 분할을 실시함으로써 세그먼트 크기를 적절하게 맞출 수 있다.
아래는 1시간 단위로 분할된 데이터 테이블을 예시로 나타낸 것이다.

이와 같이 시간 단위로 세그먼트를 구분하는 것은 데이터 소스 내의 모든 이벤트에 타임스탬프가 포함되기 때문에 가능하다.
세그먼트는 Druid 테이블의 기본 저장 단위에 해당하며, 클러스터 내 데이터의 복제(replication) 및 분산은 세그먼트 단위로 이루어진다. 세그먼트 내 데이터는 변경할 수 없도록 되어 있다. 이렇게 함으로써 읽기와 쓰기 동작 사이에 경합이 발생하지 않게 된다. Druid의 세그먼트는 매우 신속하게 읽히기 위한 읽기 전용 데이터셋이다.
뿐만 아니라, 이러한 데이터 세그먼트 분할은 Druid 분산 환경에서의 병렬 처리를 위한 핵심 역할을 한다. 각 CPU가 한 번에 하나의 세그먼트를 스캔할 수 있기 때문에 데이터를 여러 세그먼트로 분할하면 이를 여러 CPU가 동시에 병렬적으로 스캔할 수 있으므로, 쿼리 결과를 신속하게 반환하고 부하를 안정적으로 분산시킬 수 있게 된다.



데이터 저장 포맷 및 인덱싱

Druid의 데이터 구조를 분석 쿼리에 최적화시키는 주요 요소 중 하나는 Druid가 데이터를 저장하는 방식이다. 본 절에서는 설명을 위해 아래의 Druid 테이블 예시를 사용한다.



1. 컬럼 기반 저장 및 인덱싱

Druid는 컬럼들을 각각 따로 저장한다. Druid가 주로 이벤트 스트림을 집계하는 데 사용된다는 점을 고려할 때, 이러한 컬럼 기반 저장 방식을 취하면 각 쿼리에 관련된 컬럼만을 로드·스캔하므로 CPU 리소스를 보다 효율적으로 사용할 수 있다. 행 기반 데이터 스토어에서는 집계 시 대상 행과 관련된 모든 컬럼을

컬럼별로 상이한 방식으로 압축할 수 있으며 그에 따라 각기 다른 인덱스를 활용함으로써 컬럼을 메모리나 디스크에 저장하는 데 드는 리소스 비용을 줄일 수 있다. 위 예에서 page, user, gender, city 컬럼은 문자열만을 포함한다. 직접 문자열들을 저장하는 것은 불필요한 비용을 발생시키므로 이들을 고유한 정수 식별자로 매핑할 수 있다. 예:

    Justin Bieber -> 0
    Ke$ha -> 1

이 매핑을 사용하면 page 컬럼을 정수 배열로 나타낼 수 있는데, 여기서 배열 인덱스 각각은 원본 데이터셋의 각 행에 해당한다. page 컬럼의 경우, 각 행의 page 값을 아래와 같이 표시할 수 있다.

    [0, 0, 1, 1]

이처럼 문자열들이 고정 길이 정수들로 바뀌어 저장되므로 압축하기가 훨씬 더 수월하다. Druid는 각 shard(세그먼트) 단위로 데이터를 인덱싱한다.


2. 데이터 필터링을 위한 인덱싱

Druid는 검색 인덱스를 추가로 만들어서 문자열 컬럼에 대한 필터링을 용이하게 할 수 있다. 위 예시 테이블을 다시 보자. 가령 “샌프란시스코에 사는 남성 사용자들이 Wikipedia 편집을 한 횟수는?”과 같은 쿼리가 있을 수 있다. 이 쿼리 예시에는 도시(San Francisco)와 성별(Male)이라는 두 가지 차원이 포함된다. 각 차원별로 아래와 같은 바이너리 배열이 생성되는데, 여기서 배열 인덱스 각각은 해당 행이 쿼리 필터 조건에 부합하는지 여부를 나타낸다.

    San Francisco (City) -> rows [1] -> [1][0][0][0]
    Male (Gender) -> rows [1, 2, 3, 4] -> [1][1][1][1]

그런 다음 쿼리 필터는 이러한 두 배열에 대해 AND 연산을 실시한다.

    [1][0][0][0] AND [1][1][1][1] = [1][0][0][0]

그 결과, 행 1만 스캔 대상이 된다. 이런 식으로 필터링된 행만 검색함으로써 불필요한 부하를 방지하는 것이다. 이러한 바이너리 배열은 압축하기도 매우 쉽다.
이러한 검색 인덱싱은 OR 연산에도 사용할 수 있다. 어떤 쿼리가 San Francisco 또는 Calgary을 필터링하는 경우, 배열 인덱스들은 차원값별로 다음과 같을 것이다.

    San Francisco (City) -> rows [1] -> [1][0][0][0]
    Calgary (City) -> rows [3] -> [0][0][1][0]

그런 다음 두 배열에 대해 OR 연산이 수행된다.

    [1][0][0][0] OR [0][0][1][0] = [1][0][1][0]

그 결과, 쿼리는 행 1과 3만 스캔한다.
대형 비트맵 셋에 boolean 연산을 실시하는 이러한 접근방식은 검색 엔진에서 널리 사용된다.




쿼리 언어
===================================

Druid의 네이티브 쿼리 언어는 JSON over HTTP이며, 주요 쿼리는 다음과 같다.

* Group By
* 시계열 기반 roll-up
* 임의적 boolean 필터링
* Sum, Min, Max, Avg 등의 집계 연산
* 차원값 검색

하지만 이 외에도 SQL을 비롯한 다양한 언어로 이루어진 쿼리 라이브러리가 생성·공유되고있다.




기본 클러스터 아키텍쳐
===================================
Druid 클러스터는 여러 유형의 노드군으로 구성되며, 각 유형의 노드군별로 고유의 역할을 수행한다.



1. Real-time 노드

real-time 노드군은 이벤트 스트림을 ingestion하고 쿼리하는 기능을 한다. 이 노드들은 최근 발생한 짧은 시간 범위 내 이벤트들만을 처리하며, 주기적으로 이들을 딥 스토리지로 넘기는데, 그 절차는 다음과 같다.



    1. 유입되는 이벤트들은 메모리에 인덱싱되면서 즉시 쿼리에 사용될 수 있다.
    2. 메모리 상의 데이터는 정기적으로 디스크에 저장되면서 수정 불가능한(읽기 전용) 컬럼형 포맷으로 변환된다.
    3. 저장된 데이터는 off-heap 메모리로 로드되기 때문에 쿼리 가능한 상태가 유지된다.
    4. 디스크에 저장된 인덱스들을 주기적으로 병합되어 데이터 ‘세그먼트’를 구성한 후 딥 스토리지로 이관된다.

이런 식으로 real-time 노드로 ingestion된 모든 이벤트는 디스크 저장 전후를 막론하고 on-heap 또는 off-heap 메모리 상에 존재하므로 쿼리가 가능한 상태를 유지한다(쿼리는 메모리 상의 인덱스와 디스크에 저장된 인덱스 모두에 전달된다). 이러한 real-time 노드 기능을 통해 Druid는 실시간 데이터 ingestion을 수행할 수 있다. 즉, 이벤트들이 발생하면 곧 이어서 쿼리 대상이 된다. 그리고 이러한 과정에서 데이터 손실이 발생하지 않는다.
real-time 노드는 Druid 클러스터 내 다른 노드들과의 유기적인 동작을 위해 자신의 온라인 상태와 처리 중인 데이터를 Zookeeper(Druid 클러스터의 외부 종속 모듈, 3.7.5절 참조)에 보고한다.





2. Historical 노드

historical 노드군은 real-time 노드가 생성한 읽기 전용 데이터 블록(세그먼트)을 로드하고 처리하는 기능을 한다. 이 노드들은 딥 스토리지에서 읽기 전용 세그먼트를 다운로드하고 이에 대한 쿼리를 처리한다(예: 데이터 집계/필터링). 이 노드들은 shared-nothing 아키텍쳐에 기반하며 동작이 단순하다. 이들 간에는 경합이 발생하지 않으며 단순히 Zookeeper의 지시에 따라 세그먼트를 로드, 드롭, 처리할 뿐이다.
historical 노드가 쿼리를 처리하는 프로세스는 다음과 같다.



쿼리를 받으면 historical 노드는 우선 자신에게 이미 어떤 세그먼트가 존재하는지에 관한 정보를 보관하는 로컬 캐시를 확인한다. 어떤 세그먼트에 관한 정보가 캐시에 없으면 노드는 딥 스토리지에서 해당 세그먼트를 다운로드한다. 그런 다음, 해당 세그먼트는 Zookeeper에서 선언되어 쿼리가 가능한 대상이 되며, 노드는 이 세그먼트에 대해 요청된 쿼리를 수행한다.
historical 노드는 읽기 전용 데이터만을 다루므로 read consistency를 보장할 수 있다. 읽기 전용 데이터 블록들은 또한 단순한 병렬 모델을 가능케 한다. 즉, historical 노드들은 읽기 전용 데이터 블록들을 서로 간섭하지 않고 동시에 스캔·집계할 수 있다.
real-time 노드와 마찬가지로 historical 노드들도 자신들의 온라인 상태와 처리 중인 데이터를 Zookeeper에 보고한다.





3. Broker 노드

broker 노드군은 Zookeeper에 보고된 메타데이터를 통해 어떤 세그먼트들이 쿼리 가능한지와 이 세그먼트들이 각각 어디에 저장되어 있는지를 파악한다. broker 노드들은 입력된 쿼리들의 경로를 지정함으로써 각 쿼리가 올바른 historical 또는 real-time 노드에 도달되게끔 한다. 그런 다음 historical 및 real-time 노드 각각에서 산출된 결과들을 취합하여 최종 쿼리 결과를 호출자에게 반환한다.
broker 노드는 리소스 효율성을 높이기 위해 다음과 같이 캐시를 사용한다.


어떤 쿼리가 여러 세그먼트를 포괄할 경우 broker 노드는 캐시에 이미 존재하는 세그먼트들을 우선 확인한다. 그리고 캐시에 없는 세그먼트들에 대해서는 그것이 보관된 historical 및 real-time 노드로 쿼리를 전달한다. historical 노드들이 결과를 반환하면, broker 노드는 이 결과를 나중에 사용할 수 있도록 세그먼트별로 캐시에 저장한다. real-time 노드의 데이터는 캐시에 저장되지 않으며, 따라서 real-time 데이터에 대한 요청은 항상 real-time 노드로 전달된다. real-time 노드의 데이터는 가변적이기 때문에 그 결과를 캐시에 저장하는 것은 안정적이지 않기 때문이다.

4. Coordinator 노드

coordinator 노드군은 주로 historical 노드 데이터의 관리 및 분산을 담당한다. coordinator 노드는 어떤 historical 노드가 어떤 세그먼트에 대해 쿼리를 수행할지 결정하고 이들에게 새 데이터를 로드하고, 기한이 지난 데이터를 드롭하고, 데이터를 복제하고, 데이터를 이동하여 부하 밸런스를 맞추도록 지시한다. 이렇게 함으로써 분산형 historical 노드 그룹에서 빠르고 효율적이며 안정으로 데이터를 처리할 수 있다.
다른 모든 Druid 노드와 마찬가지로, coordinator 노드들도 Zookeeper 연결을 유지함으로써 클러스터의 현황을 파악한다. coordinator 노드들은 MySQL 데이터베이스와의 연결도 유지하는데, 이 데이터베이스에서는 클러스터 내 세그먼트의 생성, 소멸, 복제 규칙과 같은 추가적인 연산 매개변수 및 구성 정보를 관리한다.
Druid 클러스터의 안정성을 위해 coordinator 노드는 이중화되며 일반적으로 하나의 coordinator 노드만 활성 상태를 유지한다.

1. 외부 종속 모듈

Druid는 클러스터 동작을 위해 몇 가지 외부 종속 모듈을 사용한다.

* Zookeeper: Druid는 Zookeeper를 통해 클러스터 내부 통신을 한다.
* 메타데이터 스토리지: Druid는 메타데이터 스토리지를 통해 데이터 세그먼트 및 구성에 관한 메타데이터를 저장한다. 메타데이터 스토리지로는 주로 MySQL과 PostgreSQL이 사용된다.
* 딥 스토리지: Druid 세그먼트들을 영구적으로 백업 저장하는 공간이다. Druid에 ingestion되는 데이터는 세그먼트 형태로 딥 스토리지에 업로드되고, historical 노드들이 필요한 세그먼트를 여기서 다운로드한다. 딥 스토리지로는 주로 S3 및 HDFS가 사용된다.

2. High Availability 특성

Druid는 어느 한 노드가 고장난다고 해서 클러스터의 동작이 중단되지 않도록 설계되었다. 또한 서로 다른 유형의 노드군끼리도 상호 간에 상당히 독립적이기 때문에, 클러스터 내부에 통신 장애가 생겨도 데이터 가용성에는 최소한의 영향을 미친다. Druid 클러스터에서 highly availability를 확보하려면, 노드군별로 2개 이상의 노드가 구성되어야 한다.

3. 아키텍쳐 확장성

Druid는 위에서 소개한 기본 아키텍쳐에 다양한 외부 모듈을 추가할 수 있는 모듈 확장형 플랫폼을 지향한다. 아래는 Druid의 확장성을 활용한 모듈 조합의 예시이다.

본서에서 이후에 소개할 metatron 역시 비즈니스 인텔리전스를 위한 end-to-end 솔루션으로서 기능하기 위해 Druid 엔진 전후단에 다양한 모듈을 추가한 것이다